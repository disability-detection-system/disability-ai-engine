"""
Comprehensive Model Testing Suite - FIXED VERSION
"""

import sys
import os
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import json
from datetime import datetime

# FIXED: Get the correct project root
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

from ml.data_generator import SyntheticDataGenerator
from ml.feature_engineering import FeatureEngineer
from ml.disability_predictor import DisabilityPredictor
from ml.recommendation_engine import RecommendationEngine


class ModelValidator:
    """
    Comprehensive model validation and testing
    """
    
    def __init__(self, models_dir=None, output_dir='tests/results'):
        # FIXED: Handle models_dir path correctly
        if models_dir is None:
            models_dir = os.path.join(PROJECT_ROOT, 'models')
        
        self.models_dir = models_dir
        self.output_dir = os.path.join(PROJECT_ROOT, output_dir)
        os.makedirs(self.output_dir, exist_ok=True)
        
        self.engineer = FeatureEngineer()
        self.predictor = None
        self.test_results = {}
        
        print(f"Project Root: {PROJECT_ROOT}")
        print(f"Models Directory: {self.models_dir}")
        print(f"Output Directory: {self.output_dir}")
    
    def find_best_model(self):
        """
        FIXED: Find the best model even if best_model.txt doesn't exist
        """
        best_model_path = os.path.join(self.models_dir, 'best_model.txt')
        
        # Try to read best_model.txt
        if os.path.exists(best_model_path):
            print(f"\n✓ Found best_model.txt")
            with open(best_model_path, 'r') as f:
                lines = f.readlines()
                model_file = lines[1].split(': ')[1].strip()
            return model_file
        
        # If best_model.txt doesn't exist, find latest model file
        print(f"\n⚠ best_model.txt not found, searching for model files...")
        
        if not os.path.exists(self.models_dir):
            raise FileNotFoundError(f"Models directory not found: {self.models_dir}")
        
        # List all .pkl files (excluding preprocessor files)
        model_files = [
            f for f in os.listdir(self.models_dir)
            if f.endswith('.pkl') and 
            'scaler' not in f and 
            'label_encoder' not in f and 
            'feature_selector' not in f
        ]
        
        if not model_files:
            raise FileNotFoundError(
                f"No trained model files found in {self.models_dir}\n"
                f"Please run: python train_models.py"
            )
        
        # Sort by modification time (most recent first)
        model_files_with_time = [
            (f, os.path.getmtime(os.path.join(self.models_dir, f)))
            for f in model_files
        ]
        model_files_with_time.sort(key=lambda x: x[1], reverse=True)
        
        latest_model = model_files_with_time[0][0]
        model_path = os.path.join(self.models_dir, latest_model)
        
        print(f"✓ Found latest model: {latest_model}")
        print(f"Available models: {len(model_files)}")
        for i, (mf, _) in enumerate(model_files_with_time[:5], 1):
            print(f"  {i}. {mf}")
        
        return model_path
    
    def load_model(self, model_path=None):
        """Load trained model"""
        if model_path is None:
            model_path = self.find_best_model()
        
        # Check if path is absolute or relative
        if not os.path.isabs(model_path):
            model_path = os.path.join(self.models_dir, model_path)
        
        print(f"\nLoading model from: {model_path}")
        
        # Load preprocessors
        self.engineer.load_preprocessors(self.models_dir)
        
        # Load model
        self.predictor = DisabilityPredictor()
        self.predictor.load_model(model_path)
        
        print(f"✓ Model loaded successfully")
    
    # ... rest of the methods remain the same ...
    
    def generate_test_data(self, n_samples=500):
        """Generate test dataset"""
        print(f"\n{'='*60}")
        print("Generating Test Dataset")
        print(f"{'='*60}")
        
        generator = SyntheticDataGenerator(seed=999)
        test_data = generator.generate_combined_dataset(n_samples_per_condition=n_samples)
        
        print(f"Total test samples: {len(test_data)}")
        print(f"Distribution:\n{test_data['condition'].value_counts()}")
        
        return test_data
    
    def test_model_accuracy(self, test_data):
        """Test model accuracy on test set"""
        print(f"\n{'='*60}")
        print("Testing Model Accuracy")
        print(f"{'='*60}")
        
        X_test, y_test, _ = self.engineer.prepare_features(test_data, is_training=True)
        
        y_pred = self.predictor.predict(X_test)
        y_pred_proba = self.predictor.predict_proba(X_test)
        
        from sklearn.metrics import accuracy_score, precision_recall_fscore_support
        
        accuracy = accuracy_score(y_test, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_test, y_pred, average='weighted'
        )
        
        label_names = self.engineer.label_encoder.classes_
        report = classification_report(y_test, y_pred, target_names=label_names, output_dict=True)
        cm = confusion_matrix(y_test, y_pred)
        
        results = {
            'overall_accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'per_class_metrics': report,
            'confusion_matrix': cm.tolist(),
            'test_samples': len(X_test)
        }
        
        print(f"\nOverall Accuracy: {accuracy:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall: {recall:.4f}")
        print(f"F1-Score: {f1:.4f}")
        
        print(f"\nPer-Class Performance:")
        for label in label_names:
            print(f"\n{label.upper()}:")
            print(f"  Precision: {report[label]['precision']:.4f}")
            print(f"  Recall: {report[label]['recall']:.4f}")
            print(f"  F1-Score: {report[label]['f1-score']:.4f}")
            print(f"  Support: {report[label]['support']}")
        
        print(f"\nConfusion Matrix:")
        print(cm)
        
        self._plot_confusion_matrix(cm, label_names)
        
        self.test_results['accuracy_test'] = results
        return results
    
    def test_edge_cases(self):
        """Test model behavior on edge cases"""
        print(f"\n{'='*60}")
        print("Testing Edge Cases")
        print(f"{'='*60}")
        
        edge_cases = {
            'extreme_dyslexia': {
                'avg_letter_size': 1000, 'line_straightness': 30, 'letter_spacing': 20,
                'word_spacing': 40, 'writing_pressure': 40, 'letter_formation_quality': 25,
                'slant_angle': 30, 'consistency_score': 20, 'contour_count': 30,
                'aspect_ratio': 2.0, 'reading_speed_wpm': 25, 'pause_frequency': 4.0,
                'average_pause_duration': 2.0, 'pronunciation_score': 30,
                'fluency_score': 25, 'volume_consistency': 40, 'pitch_variation': 35,
                'speech_clarity': 30, 'word_count': 20, 'total_duration': 80, 'age': 8
            },
            'extreme_dysgraphia': {
                'avg_letter_size': 2500, 'line_straightness': 20, 'letter_spacing': 60,
                'word_spacing': 120, 'writing_pressure': 20, 'letter_formation_quality': 15,
                'slant_angle': 40, 'consistency_score': 15, 'contour_count': 25,
                'aspect_ratio': 3.0, 'reading_speed_wpm': 80, 'pause_frequency': 0.5,
                'average_pause_duration': 0.3, 'pronunciation_score': 75,
                'fluency_score': 72, 'volume_consistency': 70, 'pitch_variation': 75,
                'speech_clarity': 73, 'word_count': 70, 'total_duration': 30, 'age': 7
            },
            'borderline_normal': {
                'avg_letter_size': 1100, 'line_straightness': 65, 'letter_spacing': 30,
                'word_spacing': 55, 'writing_pressure': 65, 'letter_formation_quality': 60,
                'slant_angle': 8, 'consistency_score': 60, 'contour_count': 75,
                'aspect_ratio': 1.6, 'reading_speed_wpm': 75, 'pause_frequency': 1.0,
                'average_pause_duration': 0.5, 'pronunciation_score': 65,
                'fluency_score': 68, 'volume_consistency': 68, 'pitch_variation': 70,
                'speech_clarity': 67, 'word_count': 60, 'total_duration': 38, 'age': 9
            }
        }
        
        results = {}
        
        for case_name, features in edge_cases.items():
            print(f"\nTesting: {case_name}")
            
            input_df = pd.DataFrame([features])
            X = self.engineer.prepare_features(input_df, is_training=False)
            
            prediction = self.predictor.predict(X)[0]
            prediction_proba = self.predictor.predict_proba(X)[0]
            
            predicted_label = self.engineer.label_encoder.inverse_transform([prediction])[0]
            confidence = max(prediction_proba)
            
            prob_dict = {
                label: float(prob)
                for label, prob in zip(self.engineer.label_encoder.classes_, prediction_proba)
            }
            
            print(f"  Prediction: {predicted_label}")
            print(f"  Confidence: {confidence:.4f}")
            print(f"  Probabilities: {prob_dict}")
            
            results[case_name] = {
                'prediction': predicted_label,
                'confidence': float(confidence),
                'probabilities': prob_dict
            }
        
        self.test_results['edge_cases'] = results
        return results
    
    def test_age_sensitivity(self):
        """Test model's sensitivity to age variations"""
        print(f"\n{'='*60}")
        print("Testing Age Sensitivity")
        print(f"{'='*60}")
        
        base_features = {
            'avg_letter_size': 1200, 'line_straightness': 55, 'letter_spacing': 28,
            'word_spacing': 56, 'writing_pressure': 58, 'letter_formation_quality': 52,
            'slant_angle': 12, 'consistency_score': 48, 'contour_count': 65,
            'aspect_ratio': 1.8, 'reading_speed_wpm': 45, 'pause_frequency': 2.5,
            'average_pause_duration': 1.2, 'pronunciation_score': 45,
            'fluency_score': 40, 'volume_consistency': 55, 'pitch_variation': 48,
            'speech_clarity': 42, 'word_count': 35, 'total_duration': 55
        }
        
        ages = [6, 7, 8, 9, 10, 11, 12]
        results = []
        
        for age in ages:
            features = base_features.copy()
            features['age'] = age
            
            input_df = pd.DataFrame([features])
            X = self.engineer.prepare_features(input_df, is_training=False)
            
            prediction = self.predictor.predict(X)[0]
            prediction_proba = self.predictor.predict_proba(X)[0]
            predicted_label = self.engineer.label_encoder.inverse_transform([prediction])[0]
            
            results.append({
                'age': age,
                'prediction': predicted_label,
                'confidence': float(max(prediction_proba))
            })
            
            print(f"Age {age}: {predicted_label} (confidence: {max(prediction_proba):.4f})")
        
        self.test_results['age_sensitivity'] = results
        return results
    
    def test_performance_metrics(self, n_samples=1000):
        """Test model performance and speed"""
        print(f"\n{'='*60}")
        print("Testing Performance Metrics")
        print(f"{'='*60}")
        
        generator = SyntheticDataGenerator(seed=777)
        test_data = generator.generate_combined_dataset(n_samples_per_condition=n_samples//3)
        
        X_test, _, _ = self.engineer.prepare_features(test_data, is_training=True)
        
        import time
        
        start_time = time.time()
        predictions = self.predictor.predict(X_test)
        end_time = time.time()
        
        total_time = end_time - start_time
        avg_time_per_sample = total_time / len(X_test)
        throughput = len(X_test) / total_time
        
        print(f"\nPerformance Metrics:")
        print(f"Total samples: {len(X_test)}")
        print(f"Total time: {total_time:.4f} seconds")
        print(f"Average time per sample: {avg_time_per_sample*1000:.2f} ms")
        print(f"Throughput: {throughput:.2f} samples/second")
        
        results = {
            'total_samples': len(X_test),
            'total_time_seconds': total_time,
            'avg_time_per_sample_ms': avg_time_per_sample * 1000,
            'throughput_samples_per_second': throughput
        }
        
        self.test_results['performance'] = results
        return results
    
    def test_recommendation_engine(self):
        """Test recommendation engine integration"""
        print(f"\n{'='*60}")
        print("Testing Recommendation Engine")
        print(f"{'='*60}")
        
        rec_engine = RecommendationEngine()
        
        test_cases = [
            {
                'name': 'Mild Dyslexia',
                'prediction': 'dyslexia',
                'proba': {'normal': 0.15, 'dyslexia': 0.55, 'dysgraphia': 0.30},
                'age': 8
            },
            {
                'name': 'Severe Dysgraphia',
                'prediction': 'dysgraphia',
                'proba': {'normal': 0.05, 'dyslexia': 0.10, 'dysgraphia': 0.85},
                'age': 7
            }
        ]
        
        results = []
        
        for case in test_cases:
            print(f"\n{case['name']}:")
            
            recommendations = rec_engine.generate_recommendations(
                prediction=case['prediction'],
                prediction_proba=case['proba'],
                age=case['age']
            )
            
            print(f"  Severity: {recommendations['severity_level']}")
            print(f"  Interventions: {len(recommendations['primary_interventions'])}")
            
            results.append({
                'case': case['name'],
                'severity': recommendations['severity_level'],
                'num_interventions': len(recommendations['primary_interventions'])
            })
        
        self.test_results['recommendation_engine'] = results
        return results
    
    def _plot_confusion_matrix(self, cm, labels):
        """Plot and save confusion matrix"""
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=labels, yticklabels=labels)
        plt.title('Confusion Matrix - Test Set')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.tight_layout()
        
        filepath = os.path.join(self.output_dir, 'test_confusion_matrix.png')
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        print(f"\n✓ Confusion matrix saved to: {filepath}")
    
    def save_test_report(self):
        """Save comprehensive test report"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        json_path = os.path.join(self.output_dir, f'test_report_{timestamp}.json')
        with open(json_path, 'w') as f:
            json.dump(self.test_results, f, indent=2)
        
        print(f"\n✓ Test report saved to: {json_path}")
        
        text_path = os.path.join(self.output_dir, f'test_summary_{timestamp}.txt')
        with open(text_path, 'w') as f:
            f.write("="*80 + "\n")
            f.write("MODEL VALIDATION TEST REPORT\n")
            f.write("="*80 + "\n\n")
            f.write(f"Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            if 'accuracy_test' in self.test_results:
                f.write("ACCURACY TEST RESULTS:\n")
                f.write("-"*80 + "\n")
                acc = self.test_results['accuracy_test']
                f.write(f"Overall Accuracy: {acc['overall_accuracy']:.4f}\n")
                f.write(f"Precision: {acc['precision']:.4f}\n")
                f.write(f"Recall: {acc['recall']:.4f}\n")
                f.write(f"F1-Score: {acc['f1_score']:.4f}\n\n")
            
            if 'performance' in self.test_results:
                f.write("PERFORMANCE METRICS:\n")
                f.write("-"*80 + "\n")
                perf = self.test_results['performance']
                f.write(f"Average Time: {perf['avg_time_per_sample_ms']:.2f} ms\n")
                f.write(f"Throughput: {perf['throughput_samples_per_second']:.2f} samples/sec\n\n")
        
        print(f"✓ Test summary saved to: {text_path}")
        
        return json_path, text_path


def run_all_tests():
    """Run complete test suite"""
    print("\n" + "="*80)
    print("COMPREHENSIVE MODEL VALIDATION SUITE")
    print("="*80)
    
    validator = ModelValidator()
    
    try:
        validator.load_model()
        
        test_data = validator.generate_test_data(n_samples=500)
        validator.test_model_accuracy(test_data)
        validator.test_edge_cases()
        validator.test_age_sensitivity()
        validator.test_performance_metrics()
        validator.test_recommendation_engine()
        
        validator.save_test_report()
        
        print("\n" + "="*80)
        print("✓ ALL TESTS COMPLETED")
        print("="*80 + "\n")
        
    except Exception as e:
        print(f"\n✗ Error: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    run_all_tests()
